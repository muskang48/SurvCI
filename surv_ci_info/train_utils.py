# -*- coding: utf-8 -*-
"""train_utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aEtJyLNPvsKPEXzS38U99quwPy8GYcsT
"""

import torch 
import pdb
import numpy as np
import gc
import matplotlib.pyplot as plt
import logging
from tqdm import tqdm
from copy import deepcopy
from surv_ci_info.losses import conditional_loss,unconditional_loss,mse_loss,imb_loss,l2_loss
from surv_ci_info.utilities import get_optimizer
from surv_ci_info.model import survci_info
from torch.utils.tensorboard import SummaryWriter

np.random.seed(1234)
torch.manual_seed(seed=1234)


# def pretrain(model, t_train, c_train,e_train, W_train, t_valid,c_valid, e_valid,W_valid,
#                  n_iter=10000, lr=1e-2, thres=1e-4):
#   '''Pretrain Model to get Prior Parameters for the Distribution'''

#   premodel = survci_info(1, 1,dist=model.dist,num_treatments=model.num_treatments,optimizer=model.optimizer)

#   premodel.double()

#   optimizer = get_optimizer(premodel, lr)

#   oldcost = float('inf')
#   patience = 0
#   costs = []
#   for _ in tqdm(range(n_iter)):

#     optimizer.zero_grad()
#     loss = 0
    
#     loss = unconditional_loss(premodel, t_train,c_train, e_train, W_train)
#     # mse_loss = unconditional_mse(premodel, t_train, e_train, W_train)
#     # loss = ll_loss + premodel.p_beta*mse_loss
#     loss.backward()
#     optimizer.step()

#     valid_loss = 0
   
#     valid_loss = unconditional_loss(premodel, t_valid, c_valid, e_valid, W_valid)
#     # valid_mse = unconditional_mse(premodel,t_valid, e_valid, W_valid)
#     # valid_loss = valid_ll_loss + premodel.p_beta*valid_mse
#     valid_loss = valid_loss.detach().cpu().numpy()
#     costs.append(valid_loss)

#     if np.abs(costs[-1] - oldcost) < thres:
#       patience += 1
#       if patience == 3:
#         break

#     oldcost = costs[-1]

#   return premodel

def pretrain(model, y_train,e_train, W_train, y_valid, e_valid,W_valid,
                 n_iter=10, lr=1e-2, thres=1e-4):
  '''Pretrain Model to get Prior Parameters for the Distribution'''

  premodel = survci_info(1, 1,dist=model.dist,num_treatments=model.num_treatments,optimizer=model.optimizer)

  premodel.double().cuda()

  optimizer = get_optimizer(premodel, lr)

  oldcost = float('inf')
  patience = 0
  costs = []
  writer = SummaryWriter("pretrain_runs/")
  for i in tqdm(range(n_iter)):

    optimizer.zero_grad()
    loss = 0
    
    loss = unconditional_loss(premodel, y_train.cuda(), e_train.cuda(), W_train.cuda())
    # mse_loss = unconditional_mse(premodel, t_train, e_train, W_train)
    # loss = ll_loss + premodel.p_beta*mse_loss
    loss.backward()
    optimizer.step()

    valid_loss = 0
   
    valid_loss = unconditional_loss(premodel, y_valid.cuda(), e_valid.cuda(), W_valid.cuda())
    # valid_mse = unconditional_mse(premodel,t_valid, e_valid, W_valid)
    # valid_loss = valid_ll_loss + premodel.p_beta*valid_mse
    writer.add_scalar("pretrain/loss", loss, i)
    writer.add_scalar("pretrain/valid_loss", valid_loss, i)
    valid_loss = valid_loss.detach().cpu().numpy()
    costs.append(valid_loss)

    if np.abs(costs[-1] - oldcost) < thres:
      patience += 1
      if patience == 3:
        break

    oldcost = costs[-1]

  return premodel

def _reshape_tensor_with_nans(data):
  """Helper function to unroll padded RNN inputs."""
  data = data.reshape(-1)
  return data[~torch.isnan(data)]

def _get_padded_features(x):
  """Helper function to pad variable length RNN inputs with nans."""
  d = max([len(x_) for x_ in x])
  padx = []
  for i in range(len(x)):
    pads = np.nan*np.ones((d - len(x[i]),) + x[i].shape[1:])
    padx.append(np.concatenate([x[i], pads]))
  return np.array(padx)

def _get_padded_targets(t):
  """Helper function to pad variable length RNN inputs with nans."""
  d = max([len(t_) for t_ in t])
  padt = []
  for i in range(len(t)):
    pads = np.nan*np.ones(d - len(t[i]))
    padt.append(np.concatenate([t[i], pads]))
  return np.array(padt)[:, :, np.newaxis]

def train_survci_info(model,
              x_train, y_train, e_train,W_train,
              x_valid, y_valid, e_valid,W_valid,name,
              n_iter=1, lr=1e-3, elbo=True,
              bs=100): #bs=100
  """Function to train the torch instance of the model."""
#bs = 100 (ACTG-SYNTHETIC)
  logging.info('Pretraining the Underlying Distributions...')
  # For padded variable length sequences we first unroll the input and
  # mask out the padded nans.
  y_train_ = _reshape_tensor_with_nans(y_train.cuda())
  e_train_ = _reshape_tensor_with_nans(e_train.cuda())
  y_valid_ = _reshape_tensor_with_nans(y_valid.cuda())
  e_valid_ = _reshape_tensor_with_nans(e_valid.cuda())
  W_train_ = _reshape_tensor_with_nans(W_train.cuda())
  W_valid_ = _reshape_tensor_with_nans(W_valid.cuda())
  premodel = pretrain(model,
                          y_train_,
                          e_train_,
                          W_train_,
                          y_valid_,
                          e_valid_,
                          W_valid_,
                          n_iter=5000,
                          lr=1e-2, 
                          thres=1e-4) 
#lr =1e-2
  for r in range(model.num_treatments):
    model.shape[str(r+1)].data.fill_(float(premodel.shape[str(r+1)]))
    model.scale[str(r+1)].data.fill_(float(premodel.scale[str(r+1)]))
    model.shape_c[str(r+1)].data.fill_(float(premodel.shape_c[str(r+1)]))
    model.scale_c[str(r+1)].data.fill_(float(premodel.scale_c[str(r+1)]))

  #pdb.set_trace()

  model.double().cuda()
  optimizer = get_optimizer(model, lr)

  patience = 0
  oldcost = float('inf')
  bestci = 0 #change
  nbatches = int(x_train.shape[0]/bs)+1
  
  dics = []
  costs = []
  valid_cis = []
  train_ipm_loss = []
  valid_ipm_loss = []
  train_mse_loss = []
  valid_mse_loss = []
  train_ll_loss = []
  valid_ll_loss = []
  train_l2_loss = []
  valid_l2_loss = []
  train_total_loss =[]
  valid_total_loss = []
  i = 0
  writer = SummaryWriter(f'runs/p_alpha{model.p_alpha}p_beta{model.p_beta}p_gamma{model.p_gamma}p_lambda{model.p_lamda}_{name}')
  for i in tqdm(range(n_iter)):
    epoch_ll_loss = 0
    epoch_ipm_loss = 0
    epoch_mse_loss = 0
    epoch_l2_loss = 0
    epoch_total_loss = 0
    
    for j in range(nbatches):

      xb = x_train[j*bs:(j+1)*bs]
      yb = y_train[j*bs:(j+1)*bs]
      eb = e_train[j*bs:(j+1)*bs]
      wb = W_train[j*bs:(j+1)*bs]

      if xb.shape[0] == 0:
        continue

      optimizer.zero_grad()
      loss = 0
      train_ci = 0
      #pdb.set_trace()
      loss_ll = conditional_loss(model,xb,_reshape_tensor_with_nans(yb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb),elbo=elbo)
      ipm_loss = imb_loss(model,xb,_reshape_tensor_with_nans(wb))
      mse = mse_loss(model,xb,_reshape_tensor_with_nans(yb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb))
      l2 = l2_loss(model)
      #l_f = factual_loss(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(cb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb))
      #mse = mse_total(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb))
      loss = model.p_gamma*loss_ll + model.p_alpha*ipm_loss + model.p_beta*mse +model.p_lamda*l2 #Multiplied p_gamma
      #loss = loss_ll+model.p_alpha*ipm_loss + model.p_beta*l_f
      loss.backward()
      optimizer.step()

      epoch_total_loss += loss.item()
      epoch_ipm_loss += ipm_loss.item()
      epoch_ll_loss += loss_ll.item()
      epoch_mse_loss += mse.item()
      epoch_l2_loss += l2.item()
    
    # pdb.set_trace()
    # if 
    train_total_loss.append(epoch_total_loss/nbatches)
    train_ipm_loss.append(epoch_ipm_loss/nbatches)
    train_ll_loss.append(epoch_ll_loss/nbatches)
    train_mse_loss.append(epoch_mse_loss/nbatches)
    train_l2_loss.append(epoch_l2_loss/nbatches)
    valid_loss = 0 
    writer.add_scalar("train/Loss_total", epoch_total_loss/nbatches, i)
    writer.add_scalar("train/Loss_ipm",epoch_ipm_loss/nbatches, i)
    writer.add_scalar("train/Loss_ll", epoch_ll_loss/nbatches, i)
    writer.add_scalar("train/Loss_mse", epoch_mse_loss/nbatches, i)
    
    model1 = model.eval()
    valid_ll = conditional_loss(model1,x_valid,y_valid_,e_valid_,W_valid_,elbo=False)
    valid_ipm = imb_loss(model1,x_valid,W_valid_)
    valid_mse = mse_loss(model1,x_valid,y_valid_,e_valid,W_valid_)
    valid_l2 = l2_loss(model1)
    #valid_lf = factual_loss(model1,x_valid,t_valid_,e_valid_,W_valid_ )
    #valid_mse = mse_total(model,x_valid,t_valid_,e_valid_,W_valid_)
    valid_loss = model1.p_gamma*valid_ll + model1.p_alpha*valid_ipm + model1.p_beta*valid_mse +model1.p_lamda*valid_l2
    #valid_loss = valid_ll+model.p_alpha*valid_ipm + model.p_beta*valid_lf
    
    valid_total_loss.append(valid_loss.item())
    valid_ll_loss.append(valid_ll.item())
    valid_ipm_loss.append(valid_ipm.item())
    valid_mse_loss.append(valid_mse.item())
    valid_l2_loss.append(valid_l2.item())
    writer.add_scalar("valid/Loss_total", valid_loss, i)
    writer.add_scalar("valid/Loss_ipm",valid_ipm, i)
    writer.add_scalar("valid/Loss_ll", valid_ll, i)
    writer.add_scalar("valid/Loss_mse", valid_mse, i)

    print('\n--------------------------------------------------')
    print('Epoch: {}  Train Total Loss: {:.4f} Train elbo Loss: {:.4f}  Train ipm Loss: {:.4f} Train mse Loss: {:.4f}  Train L2 R Loss: {:.4f}'.format(i, loss, loss_ll, ipm_loss, mse, l2 ))
    print('--------------------------------------------------')
    print('\n--------------------------------------------------')
    print('Epoch: {}  Val Total Loss: {:.4f}  Val elbo Loss: {:.4f} Val ipm Loss: {:.4f} Val mse Loss: {:.4f}   Val L2  Loss: {:4f}'.format(i, valid_loss,valid_ll, valid_ipm,valid_mse, valid_l2))
    print('--------------------------------------------------')
    valid_loss = valid_loss.detach().cpu().numpy()
    costs.append(float(valid_loss))
    dics.append(deepcopy(model.state_dict()))

    if costs[-1] >= oldcost:
      if patience == 2:
        # save_plots(model,train_total_loss,train_ll_loss,train_ipm_loss,train_mse_loss,train_l2_loss,valid_total_loss,valid_ll_loss,valid_ipm_loss,valid_mse_loss,valid_l2_loss)
        minm = np.argmin(costs)
        model.load_state_dict(dics[minm])

        del dics
        gc.collect()
        torch.save(model.state_dict(),f"survci_info_p-alpha{model.p_alpha}_p-beta{model.p_beta}_p-gamma{model.p_gamma}_{name}.pth")
        writer.add_hparams({'Lr':lr ,'p_alpha':model.p_alpha,'p_beta':model.p_beta},{'train_total_loss':np.mean(train_total_loss),'train_ll_loss':np.mean(train_ll_loss),'train_ipm_loss':np.mean(train_ipm_loss),'train_mse_loss':np.mean(train_mse_loss)})
        return model, i
      else:
        patience += 1
    else:
      patience = 0

    oldcost = costs[-1]
  
  # save_plots(model,train_total_loss,train_ll_loss,train_ipm_loss,train_mse_loss,train_l2_loss,valid_total_loss,valid_ll_loss,valid_ipm_loss,valid_mse_loss,valid_l2_loss)
  minm = np.argmin(costs)
  model.load_state_dict(dics[minm])

  del dics
  gc.collect()
  torch.save(model.state_dict(),f"survci_info_p-alpha{model.p_alpha}_p-beta{model.p_beta}_p-gamma{model.p_gamma}_{name}.pth")
  writer.add_hparams({'Lr':lr ,'p_alpha':model.p_alpha,'p_beta':model.p_beta,"p_gamma":model.p_gamma},{'train_total_loss':np.mean(train_total_loss),'train_ll_loss':np.mean(train_ll_loss),'train_ipm_loss':np.mean(train_ipm_loss),'train_mse_loss':np.mean(train_mse_loss)})
  return model, i
# def train_survci_info(model,
#               x_train, t_train,c_train, e_train,W_train,
#               x_valid, t_valid,c_valid, e_valid,W_valid,
#               n_iter=1, lr=1e-3, elbo=True,
#               bs=100): #bs=100
#   """Function to train the torch instance of the model."""
# #bs = 100 (ACTG-SYNTHETIC)
#   logging.info('Pretraining the Underlying Distributions...')
#   # For padded variable length sequences we first unroll the input and
#   # mask out the padded nans.
#   t_train_ = _reshape_tensor_with_nans(t_train)
#   c_train_ = _reshape_tensor_with_nans(c_train)
#   e_train_ = _reshape_tensor_with_nans(e_train)
#   t_valid_ = _reshape_tensor_with_nans(t_valid)
#   c_valid_ = _reshape_tensor_with_nans(c_valid)
#   e_valid_ = _reshape_tensor_with_nans(e_valid)
#   W_train_ = _reshape_tensor_with_nans(W_train)
#   W_valid_ = _reshape_tensor_with_nans(W_valid)
#   premodel = pretrain(model,
#                           t_train_,
#                           c_train_,
#                           e_train_,
#                           W_train_,
#                           t_valid_,
#                           c_valid_,
#                           e_valid_,
#                           W_valid_,
#                           n_iter=5000,
#                           lr=1e-2, 
#                           thres=1e-4) 
# #lr =1e-2
#   for r in range(model.num_treatments):
#     model.shape[str(r+1)].data.fill_(float(premodel.shape[str(r+1)]))
#     model.scale[str(r+1)].data.fill_(float(premodel.scale[str(r+1)]))
#     model.shape_c[str(r+1)].data.fill_(float(premodel.shape_c[str(r+1)]))
#     model.scale_c[str(r+1)].data.fill_(float(premodel.scale_c[str(r+1)]))

#   #pdb.set_trace()

#   model.double()
#   optimizer = get_optimizer(model, lr)

#   patience = 0
#   oldcost = float('inf')
#   bestci = 0 #change
#   nbatches = int(x_train.shape[0]/bs)+1
  
#   dics = []
#   costs = []
#   valid_cis = []
#   train_ipm_loss = []
#   valid_ipm_loss = []
#   train_mse_loss = []
#   valid_mse_loss = []
#   train_ll_loss = []
#   valid_ll_loss = []
#   train_total_loss =[]
#   valid_total_loss = []
#   i = 0
#   for i in tqdm(range(n_iter)):
#     epoch_ll_loss = 0
#     epoch_ipm_loss = 0
#     epoch_mse_loss = 0
#     epoch_total_loss = 0
#     for j in range(nbatches):

#       xb = x_train[j*bs:(j+1)*bs]
#       tb = t_train[j*bs:(j+1)*bs]
#       cb = c_train[j*bs:(j+1)*bs]
#       eb = e_train[j*bs:(j+1)*bs]
#       wb = W_train[j*bs:(j+1)*bs]

#       if xb.shape[0] == 0:
#         continue

#       optimizer.zero_grad()
#       loss = 0
#       train_ci = 0
#       #pdb.set_trace()
#       loss_ll = conditional_loss(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(cb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb),elbo=elbo)
#       ipm_loss = imb_loss(model,xb,_reshape_tensor_with_nans(wb))
#       mse = mse_loss(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(cb),_reshape_tensor_with_nans(wb))
#       #l_f = factual_loss(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(cb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb))
#       #mse = mse_total(model,xb,_reshape_tensor_with_nans(tb),_reshape_tensor_with_nans(eb),_reshape_tensor_with_nans(wb))
#       loss = loss_ll + model.p_alpha*ipm_loss + model.p_beta*mse
#       #loss = loss_ll+model.p_alpha*ipm_loss + model.p_beta*l_f
#       loss.backward()
#       optimizer.step()

#       epoch_total_loss += loss.item()
#       epoch_ipm_loss += ipm_loss.item()
#       epoch_ll_loss += loss_ll.item()
#       epoch_mse_loss += mse.item()
    
#     # pdb.set_trace()
#     # if 
#     train_total_loss.append(epoch_total_loss/nbatches)
#     train_ipm_loss.append(epoch_ipm_loss/nbatches)
#     train_ll_loss.append(epoch_ll_loss/nbatches)
#     train_mse_loss.append(epoch_mse_loss/nbatches)
#     valid_loss = 0 
    
#     model1 = model.eval()
#     valid_ll = conditional_loss(model1,x_valid,t_valid_, c_valid_,e_valid_,W_valid_,elbo=False)
#     valid_ipm = imb_loss(model1,x_valid,W_valid_)
#     valid_mse = mse_loss(model1,x_valid,t_valid_,c_valid_,W_valid_)
#     #valid_lf = factual_loss(model1,x_valid,t_valid_,e_valid_,W_valid_ )
#     #valid_mse = mse_total(model,x_valid,t_valid_,e_valid_,W_valid_)
#     valid_loss = valid_ll + model1.p_alpha*valid_ipm + model1.p_beta*valid_mse
#     #valid_loss = valid_ll+model.p_alpha*valid_ipm + model.p_beta*valid_lf
    
#     valid_total_loss.append(valid_loss.item())
#     valid_ll_loss.append(valid_ll.item())
#     valid_ipm_loss.append(valid_ipm.item())
#     valid_mse_loss.append(valid_mse.item())

#     print('\n--------------------------------------------------')
#     print('Epoch: {}  Train Total Loss: {:.4f} Train elbo Loss: {:.4f}  Train ipm Loss: {:.4f} Train mse Loss: {:.4f}  '.format(i, loss, loss_ll, ipm_loss, mse))
#     print('--------------------------------------------------')
#     print('\n--------------------------------------------------')
#     print('Epoch: {}  Val Total Loss: {:.4f}  Val elbo Loss: {:.4f} Val ipm Loss: {:.4f} Val mse Loss: {:.4f}  '.format(i, valid_loss,valid_ll, valid_ipm,valid_mse))
#     print('--------------------------------------------------')
#     valid_loss = valid_loss.detach().cpu().numpy()
#     costs.append(float(valid_loss))
#     dics.append(deepcopy(model.state_dict()))

#     if costs[-1] >= oldcost:
#       if patience == 2:
#         save_plots(model,train_total_loss,train_ll_loss,train_ipm_loss,train_mse_loss,valid_total_loss,valid_ll_loss,valid_ipm_loss,valid_mse_loss)
#         minm = np.argmin(costs)
#         model.load_state_dict(dics[minm])

#         del dics
#         gc.collect()

#         return model, i
#       else:
#         patience += 1
#     else:
#       patience = 0

#     oldcost = costs[-1]
  
#   save_plots(model,train_total_loss,train_ll_loss,train_ipm_loss,train_mse_loss,valid_total_loss,valid_ll_loss,valid_ipm_loss,valid_mse_loss)
#   minm = np.argmin(costs)
#   model.load_state_dict(dics[minm])

#   del dics
#   gc.collect()

#   return model, i

def save_plots(model,train_total_loss,train_ll_loss,train_ipm_loss,train_mse_loss,train_l2_loss,valid_total_loss,valid_ll_loss,valid_ipm_loss,valid_mse_loss,valid_l2_loss):
  plt.plot(train_total_loss,'-o',color='blue',label='Train')
  plt.plot(valid_total_loss,'-o',color='green',label='Valid')
  plt.xlabel('Epoch')
  plt.ylabel('Total Loss')
  plt.legend(loc='best', fontsize=10)
  plt.title('Train vs Valid Total Loss')
  plt.show()
  #plt.savefig('/content/drive/MyDrive/Colab Notebooks/Thesis/causal_survival_analysis/saved_models/plots/actg_synthetic/tot_loss_K_{}_alpha_{}_beta_{}'.format(model.k, model.p_alpha,model.p_beta))
  plt.plot(train_ipm_loss,'-o',color='blue',label='Train')
  plt.plot(valid_ipm_loss,'-o',color='green',label='Valid')
  plt.xlabel('Epoch')
  plt.ylabel('IPM Loss')
  plt.legend(loc='best', fontsize=10)
  plt.title('Train vs Valid IPM Loss')
  plt.show()
  #plt.savefig('/content/drive/MyDrive/Colab Notebooks/Thesis/causal_survival_analysis/saved_models/plots/actg_synthetic/IPM_loss_K_{}_alpha_{}_beta_{}'.format(model.k, model.p_alpha,model.p_beta))
  plt.plot(train_mse_loss,'-o',color='blue',label='Train')
  plt.plot(valid_mse_loss,'-o',color='green',label='Valid')
  plt.xlabel('Epoch')
  plt.ylabel('MSE Loss')
  plt.legend(loc='best', fontsize=10)
  plt.title('Train vs Valid MSE Loss')
  plt.show()
  #plt.savefig('/content/drive/MyDrive/Colab Notebooks/Thesis/causal_survival_analysis/saved_models/plots/actg_synthetic/mse_loss_K_{}_alpha_{}_beta_{}'.format(model.k, model.p_alpha,model.p_beta))
  plt.plot(train_ll_loss,'-o',color='blue',label='Train')
  plt.plot(valid_ll_loss,'-o',color='green',label='Valid')
  plt.xlabel('Epoch')
  plt.ylabel('Negative Log Likelihood Loss')
  plt.legend(loc='best', fontsize=10)
  plt.title('Train vs Valid NLL Loss')
  plt.show()
  plt.plot(train_l2_loss,'-o',color='blue',label='Train')
  plt.plot(valid_l2_loss,'-o',color='green',label='Valid')
  plt.xlabel('Epoch')
  plt.ylabel('L2 Loss')
  plt.legend(loc='best', fontsize=10)
  plt.title('Train vs Valid L2 Regularization Loss')
  plt.show()
  #plt.savefig('/content/drive/MyDrive/Colab Notebooks/Thesis/causal_survival_analysis/saved_models/plots/actg_synthetic/NLL_loss_K_{}_alpha_{}_beta_{}'.format(model.k, model.p_alpha,model.p_beta))