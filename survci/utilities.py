# -*- coding: utf-8 -*-
"""utilities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19uSO6SzDVwHjK887O6uGLBbNTuENcGrH
"""

import torch 
import numpy as np
import torch.nn as nn
import pdb
# import gc
# import logging
# from tqdm import tqdm
# from copy import deepcopy
# from surv_ci.losses import conditional_loss,unconditional_loss,mse_loss,imb_loss
# from surv_ci.model import survci


np.random.seed(1234)
torch.manual_seed(seed=1234)

SQRT_CONST = 1e-10
def safe_sqrt(x, lbound=SQRT_CONST):
  ''' Numerically safe version of TensorFlow sqrt '''
  return torch.sqrt(torch.clip(x, lbound, np.inf))

def lindisc(X,W,p):
  ''' Linear MMD '''

  it = torch.where(W>0)[0]
  ic = torch.where(W<1)[0]
  Xc = X[ic,:]#tf.gather(X,ic

  Xt = X[it,:]#tf.gather(X,it)
          
  mean_control = torch.mean(Xc, dim=0)
  mean_treated = torch.mean(Xt, dim=0)

  p = W.sum()/float(len(W)) #Probability of treatment=1
          #p = torch.tensor(p)
  c = torch.square(2*p-1)*0.25
  f = torch.sign(p-0.5)

  mmd = torch.sum(torch.square(p*mean_treated - (1-p)*mean_control))
  mmd = f*(p-0.5) + safe_sqrt(c + mmd)

  return mmd

def pdist2sq(X,Y):
  """ Computes the squared Euclidean distance between all pairs x in X, y in Y """
  C = -2*torch.matmul(X,torch.transpose(Y,0,1))#-2*tf.matmul(X,tf.transpose(Y))
    
  nx = torch.sum(torch.mul(X,X),dim=1).view(-1,1)#tf.reduce_sum(tf.square(X),1,keep_dims=True)
  ny = torch.sum(torch.mul(Y,Y),dim=1).view(-1,1)#tf.reduce_sum(tf.square(Y),1,keep_dims=True)
    
  D = (C + torch.transpose(ny,0,1)) + nx

  return D

def mmd2_lin(X,W,p):
  ''' Linear MMD '''

  it = torch.where(W>0)[0]
  ic = torch.where(W<1)[0]

  Xc = X[ic,:]

  Xt = X[it,:]

  mean_control = torch.mean(Xc, dim=0)
  mean_treated = torch.mean(Xt, dim=0)

  mmd = torch.sum(torch.square(2.0*p*mean_treated - 2.0*(1.0-p)*mean_control))

  return mmd

######################################################################################

    
def get_parameters(model,x,w):
  ''' Output Parameters of K Primitive Distributions for Control and Treated Group'''

  shape_co, scale_co, logits_co, shape_tr, scale_tr, logits_tr = model.forward(x.cuda(), w.cuda()) 
  
  return shape_co, scale_co, logits_co, shape_tr, scale_tr, logits_tr

######################################################################################

def softmax_out(shape,scale,logits):
  '''Returns parameters of single distribution by averaging K Primitive Distributions'''
  
  squish = nn.Softmax(dim=1)
  logits = squish(logits)
  k = shape.shape[1] #Number of Primitive Distributions 
  eta_ = shape
  beta_ = scale
  
  lshape = []
  lscale = []
  for g in range(k):
    mu = eta_[:, g]
    sigma = beta_[:, g]
    lshape.append(mu)
    lscale.append(sigma)
  lshape = torch.stack(lshape, dim=1)
  lshape = lshape*logits
  lshape = torch.sum(lshape, dim=1)
  lscale = torch.stack(lscale, dim=1)
  lscale = lscale*logits
  lscale = torch.sum(lscale, dim=1)

  return lshape, lscale

######################################################################################


def sample_weibull(shape, scale, samples=200):
    '''Samples from Weibull Distribution'''
    lam = np.exp(scale)
    k = np.exp(shape)

    gen_t = [lam[i] * np.random.weibull(k[i], samples) for i in np.arange(len(shape))]
    return np.exp(gen_t)

def sample_lognormal(mu, sigma, samples =200):
    '''Samples from LogNormal Distribution'''
    sigma = np.exp(sigma)
    gen_t = [np.random.lognormal(mu[i], sigma[i], samples) for i in np.arange(len(mu))]
    return gen_t

######################################################################################

def weibull_surv(t, shape, scale):
  scale = torch.exp(scale)
  shape = torch.exp(shape)
  s = torch.pow(torch.div(t,scale), shape)
  s = torch.exp(-(s))
  return s 

def lognormal_surv(t,shape,scale):
  mu = shape
  sigma =torch.exp(scale)
  s = torch.div(torch.log(t) - mu, sigma*np.sqrt(2))
  s = 0.5 - 0.5*torch.erf(s)
  return s 

def survival_prob(model, t, shape, scale):

  if model.dist == 'Weibull':
     return weibull_surv(t,shape,scale)
  elif model.dist == 'LogNormal':
     return lognormal_surv(t,shape,scale)
  else:
     raise NotImplementedError('Distribution: '+model.dist+
                               ' not implemented yet.')

def auc_unconditional(model,t,shape,scale):
  shape = shape.expand(t.shape[0], -1)
  scale = scale.expand(t.shape[0], -1)
  #T = torch.tensor(np.arange(torch.min(t),torch.max(t),1)).clone().detach()
  T = torch.tensor(np.arange(0.001,torch.max(t),1)).clone().detach()
  #T = torch.tensor(np.arange(0.01,torch.max(t),0.01)).clone().detach()
  surv_auc = []
  for i in range(shape.shape[0]):
    shape_temp = shape[i].repeat(len(T))
    scale_temp = scale[i].repeat(len(T))
    s = survival_prob(model, T, shape_temp, scale_temp)
    surv_auc.append(torch.trapz(s,T))

  survival_auc = torch.stack(surv_auc, dim=0)
  return survival_auc

def auc(model, t, shape, scale, logits):
  '''Area Under Survival Curve '''

  shape ,scale = softmax_out(shape, scale, logits)
  #pdb.set_trace()
  #T = torch.tensor([*range(int(torch.min(t)), int(torch.max(t)))]).clone().detach()
  T = torch.tensor(torch.arange(0.001,torch.max(t),1)).clone().detach()
  #T = torch.tensor(np.arange(0.01,torch.max(t),0.01)).clone().detach()
  surv_auc = []
  for i in range(shape.shape[0]):
    shape_temp = shape[i].repeat(len(T))
    scale_temp = scale[i].repeat(len(T))
    s = survival_prob(model, T.cuda(), shape_temp, scale_temp)
    surv_auc.append(torch.trapz(s.cuda(),T.cuda()))
  #pdb.set_trace()
  survival_auc = torch.stack(surv_auc, dim=0)
  return survival_auc 

######################################################################################


def get_optimizer(model, lr):

  if model.optimizer == 'Adam':
    return torch.optim.Adam(model.parameters(), lr=lr)
  elif model.optimizer == 'SGD':
    return torch.optim.SGD(model.parameters(), lr=lr)
  elif model.optimizer == 'RMSProp':
    return torch.optim.RMSprop(model.parameters(), lr=lr)
  else:
    raise NotImplementedError('Optimizer '+model.optimizer+
                              ' is not implemented')

######################################################################################


