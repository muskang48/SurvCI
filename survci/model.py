# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWfaEroaR71NvhBrx1ZurgytnKl7lLBE
"""

import torch.nn as nn
import torch
import numpy as np
import pdb
np.random.seed(1234)
torch.manual_seed(seed=1234)

def create_representation(inputdim, layers, activation,norm):
  
  if activation == 'ReLU6':
    act = nn.ReLU6()
  elif activation == 'ReLU':
    act = nn.ReLU()
  elif activation == 'SeLU':
    act = nn.SELU()

  modules = []
  prevdim = inputdim

  
  for i in range(len(layers)):
    modules.append(nn.Linear(prevdim, layers[i], bias=False))
    # if i == len(layers)-1:
    if norm=="Batch":
      modules.append(nn.BatchNorm1d(layers[i]))
    elif norm=="Layer":
      modules.append(nn.LayerNorm(layers[i]))
    else:
      pass
    modules.append(act)
    #modules.append(nn.Dropout(p=0.2))
    prevdim = layers[i]
    #batch norm

  return nn.Sequential(*modules)



class survci(nn.Module):

  def _init_survci_layers(self, lastdim):

    if self.dist in ['Weibull']:
      self.act = nn.SELU()
      self.shape = nn.ParameterDict({str(r+1): nn.Parameter(-torch.ones(self.k))
                                     for r in range(self.num_treatments)})
      self.scale = nn.ParameterDict({str(r+1): nn.Parameter(-torch.ones(self.k))
                                     for r in range(self.num_treatments)})
    elif self.dist in ['Normal']:
      self.act = nn.Identity()
      self.shape = nn.ParameterDict({str(r+1): nn.Parameter(torch.ones(self.k))
                                     for r in range(self.num_treatments)})
      self.scale = nn.ParameterDict({str(r+1): nn.Parameter(torch.ones(self.k))
                                     for r in range(self.num_treatments)})
    elif self.dist in ['LogNormal']:
      self.act = nn.Tanh()
      self.shape = nn.ParameterDict({str(r+1): nn.Parameter(torch.ones(self.k))
                                     for r in range(self.num_treatments)})
      self.scale = nn.ParameterDict({str(r+1): nn.Parameter(torch.ones(self.k))
                                     for r in range(self.num_treatments)})
    else:
      raise NotImplementedError('Distribution: '+self.dist+' not implemented'+
                                ' yet.')

    self.gate = nn.ModuleDict({str(r+1): nn.Sequential(
        nn.Linear(lastdim, self.k, bias=False)
        ) for r in range(self.num_treatments)})

    self.scaleg = nn.ModuleDict({str(r+1): nn.Sequential(
        nn.Linear(lastdim, self.k, bias=True)
        ) for r in range(self.num_treatments)})

    self.shapeg = nn.ModuleDict({str(r+1): nn.Sequential(
        nn.Linear(lastdim, self.k, bias=True)
        ) for r in range(self.num_treatments)})

  def __init__(self, inputdim, k, layers=None, dist='LogNormal',
               temp=1000., discount=1.0, optimizer='Adam',
               num_treatments=2,imb_func='lin_disc',p_ipm=0.2,p_alpha=1e-2, p_beta = 1e-4,p_lambda=1e-1):
    super(survci, self).__init__()

    self.k = k
    self.dist = dist
    self.temp = float(temp)
    self.discount = float(discount)
    self.optimizer = optimizer
    self.num_treatments = num_treatments
    self.imb_func = imb_func
    self.p_ipm = p_ipm
    self.p_alpha = p_alpha
    self.p_beta = p_beta
    self.p_lamda = p_lambda

    if layers is None:
      layers = []
    self.layers = layers

    if len(layers) == 0:
      lastdim = inputdim
    else:
      lastdim = layers[-1]

    self._init_survci_layers(lastdim)

    self.embedding = create_representation(inputdim, layers, 'ReLU6',norm="")



  def forward(self,x,W):
    xrep = self.embedding(x)
    beta_co = torch.ones((xrep[W==0].shape[0], self.k),dtype=torch.float64) #change
    eta_co = torch.ones((xrep[W==0].shape[0], self.k),dtype=torch.float64) #change
    logits_co = torch.ones((xrep[W==0].shape[0], self.k),dtype=torch.float64) #change
    beta_tr = torch.ones((xrep[W==1].shape[0], self.k),dtype=torch.float64) #change
    eta_tr = torch.ones((xrep[W==1].shape[0], self.k),dtype=torch.float64) #change
    logits_tr = torch.ones((xrep[W==1].shape[0], self.k),dtype=torch.float64) #change
    
    xrep_co = xrep[W==0]
    xrep_tr = xrep[W==1]
    dim0 = xrep_co.shape[0]
    dim1 = xrep_tr.shape[0]
    eta_co,beta_co,logits_co =  self.act(self.shapeg[str(1)](xrep_co))+self.shape[str(1)].expand(dim0, -1),self.act(self.scaleg[str(1)](xrep_co))+self.scale[str(1)].expand(dim0, -1),self.gate[str(1)](xrep_co)/self.temp
    eta_tr,beta_tr,logits_tr =  self.act(self.shapeg[str(2)](xrep_tr))+self.shape[str(2)].expand(dim1, -1),self.act(self.scaleg[str(2)](xrep_tr))+self.scale[str(2)].expand(dim1, -1),self.gate[str(2)](xrep_tr)/self.temp
    return(eta_co,beta_co,logits_co,eta_tr,beta_tr,logits_tr)
    


  def get_shape_scale(self,W):
    num_tr = int(W.sum())
    num_co = len(W) - num_tr
    shape_co = torch.ones((num_co,self.k), dtype=torch.float64) #change
    scale_co = torch.ones((num_co,self.k),dtype=torch.float64) #change
    shape_tr = torch.ones((num_tr,self.k), dtype=torch.float64) #change
    scale_tr = torch.ones((num_tr,self.k),dtype=torch.float64) #change
    shape_co = self.shape[str(1)]
    scale_co = self.scale[str(1)]
    shape_tr = self.shape[str(2)]
    scale_tr = self.scale[str(2)]
    return(shape_co,scale_co,shape_tr,scale_tr)


  def get_repr(self,x):
      return (self.embedding(x))
  
  def get_layers(self):
    #return (self.modules)
    return self.embedding