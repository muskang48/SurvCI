{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/muskang48/SurvCI/blob/main/surv_ci_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhaWjhU0nOQi",
    "outputId": "4ceefb89-0b62-4e75-a267-445cb4b5e3c6",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "S-3 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "s3_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[0, 1]_0.33.npz\")\n",
    "s3_train_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[0, 1]_0.npz\")\n",
    "s3_train_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[0, 1]_1.npz\")\n",
    "s3_train_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[8, 9]_0.33.npz\")\n",
    "s3_train_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[8, 9]_0.npz\")\n",
    "s3_train_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s3_[8, 9]_1.npz\")\n",
    "\n",
    "s3_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[0, 1]_0.33.npz\")\n",
    "s3_test_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[0, 1]_0.npz\")\n",
    "s3_test_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[0, 1]_1.npz\")\n",
    "s3_test_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[8, 9]_0.33.npz\")\n",
    "s3_test_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[8, 9]_0.npz\")\n",
    "s3_test_6 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s3_[8, 9]_1.npz\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [s3_train_2]\n",
    "test_list = [s3_test_2]\n",
    "names = [\"s3_[0, 1]_1\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-4 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "s4_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_0.33.npz\")\n",
    "s4_train_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_0.npz\")\n",
    "s4_train_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_1.npz\")\n",
    "s4_train_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_0.33.npz\")\n",
    "s4_train_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_0.npz\")\n",
    "s4_train_6 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_1.npz\")\n",
    "\n",
    "s4_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_0.33.npz\")\n",
    "s4_test_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_0.npz\")\n",
    "s4_test_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_1.npz\")\n",
    "s4_test_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_0.33.npz\")\n",
    "s4_test_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_0.npz\")\n",
    "s4_test_6 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_1.npz\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [s4_train_2]\n",
    "test_list = [s4_test_2]\n",
    "names = [\"s4_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-1 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "s1_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s1.npz\")\n",
    "s1_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s1.npz\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [s1_train_1]\n",
    "test_list = [s1_test_1]\n",
    "names = [\"s1_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-2 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "s2_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s2.npz\")\n",
    "s2_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s2.npz\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [s2_train_1]\n",
    "test_list = [s2_test_1]\n",
    "names = [\"s2_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "actg_s3_train_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/train/event_pairs.csv\")\n",
    "actg_s3_test_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/test/event_pairs.csv\")\n",
    "actg_s3_val_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/val/event_pairs.csv\")\n",
    "actg_s3_train_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/train/covariates.npy\")\n",
    "actg_s3_test_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/test/covariates.npy\")\n",
    "actg_s3_val_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/val/covariates.npy\")\n",
    "actg_s3_train_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/train/treatment.npy\")\n",
    "actg_s3_test_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/test/treatment.npy\")\n",
    "actg_s3_val_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s3/val/treatment.npy\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [[actg_s3_train_gt,actg_s3_train_x,actg_s3_train_a]]\n",
    "test_list = [[actg_s3_test_gt,actg_s3_test_x,actg_s3_test_a]]\n",
    "val_list = [[actg_s3_val_gt,actg_s3_val_x,actg_s3_val_a]]\n",
    "names = [\"actg_s3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "actg_s4_train_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/event_pairs.csv\")\n",
    "actg_s4_test_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/event_pairs.csv\")\n",
    "actg_s4_val_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/event_pairs.csv\")\n",
    "actg_s4_train_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/covariates.npy\")\n",
    "actg_s4_test_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/covariates.npy\")\n",
    "actg_s4_val_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/covariates.npy\")\n",
    "actg_s4_train_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/treatment.npy\")\n",
    "actg_s4_test_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/treatment.npy\")\n",
    "actg_s4_val_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/treatment.npy\")\n",
    "\n",
    "# add data setting need to be tested in the below list\n",
    "train_list = [[actg_s4_train_gt,actg_s4_train_x,actg_s4_train_a]]\n",
    "test_list = [[actg_s4_test_gt,actg_s4_test_x,actg_s4_test_a]]\n",
    "val_list = [[actg_s4_val_gt,actg_s4_val_x,actg_s4_val_a]]\n",
    "names = [\"actg_s4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwZsXyIlrbFY"
   },
   "source": [
    "\n",
    "\n",
    "# SurvCI Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlPz3INDnve0",
    "outputId": "1771816d-dd74-47c9-8415-762212c1fe35",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iiitd/gokul/intern/SurvCI/survci\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/surv_ci\n",
    "%cd /home/iiitd/gokul/intern/SurvCI/survci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C7T5sHfGnxKi",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Importing Model\n",
    "from survci.survci_api import survciBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG data Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "#k,p_alpha,p_beta,\n",
    "p_alpha=[0.5,0.6,0.7]\n",
    "p_beta=[1e-4,2e-4,3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "k = [3]\n",
    "\n",
    "for i,j,k,l in zip(train_list,test_list,val_list,names):\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(i[1])\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(j[1])\n",
    "    x_val = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(k[1])\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    x_val = StandardScaler().fit_transform(x_val)\n",
    "    # train\n",
    "    tf_train = i[0]['y_f']\n",
    "    tcf_train = i[0]['y_cf']\n",
    "    ef_train = i[0]['e_f']\n",
    "    ecf_train = i[0]['e_cf']\n",
    "    W_train = i[2]\n",
    "    # test\n",
    "    tf_test = j[0]['y_f']\n",
    "    tcf_test = j[0]['y_cf']\n",
    "    ef_test = j[0]['e_f']\n",
    "    ecf_test = j[0]['e_cf']\n",
    "    W_test = j[2]\n",
    "    # val\n",
    "    tf_val = k[0]['y_f']\n",
    "    tcf_val = k[0]['y_cf']\n",
    "    ef_val = k[0]['e_f']\n",
    "    ecf_val = k[0]['e_cf']\n",
    "    W_val = k[2]\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    ci=[]\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=l)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, np.array(tf_train), np.array(ef_train),W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, np.array(tf_val), np.array(ef_val),W_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JB-HSY2d1g4I",
    "outputId": "3d61fe0c-9eb9-4ceb-e00f-7ccf8b8cb835",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "#k,p_alpha,p_beta,\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j[\"t\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_test))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    rand3 = np.random.randint(0,len(W_val),20)\n",
    "    rand4 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j,k,l in zip(rand1,rand2,rand3,rand4):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "        ef_val[k] = 0\n",
    "        ef_val[l] = 0\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e\"]\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j[\"t\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e\"]\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    # tf_train = i['t']\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t_f']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S4 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e_f\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t_f']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e_f\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_test))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    print(x_train.shape)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ep7ZELsBK2Q"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from survci.model import survci\n",
    "import os\n",
    "import glob\n",
    "model_paths=glob.glob(os.path.join(\"/home/iiitd/gokul/intern/SurvCI/survci/\",'*.pth'))\n",
    "# model_paths = [\"/home/iiitd/gokul/intern/SurvCI/survci/survci_p-alpha0.5_p-beta0.0001_0.2_actg_s4.pth\"]\n",
    "def load_model(model_path):\n",
    "    # alpha=float(model_path.split('_')[1])\n",
    "    # beta=float(model_path.split('_')[2].split('pth')[0][0:-1])\n",
    "    model_test=survci(x_test.shape[-1],k=3,layers=[100,100],p_alpha=0.5,p_beta=0.0001,p_lambda=0.2)\n",
    "    model_test.double().cuda()\n",
    "    model_test.eval()\n",
    "    model_test.load_state_dict(torch.load(model_path))\n",
    "    return model_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from survci.utilities import lognormal_surv,softmax_out\n",
    "def auc(t, shape, scale, logits):\n",
    "  '''Area Under Survival Curve '''\n",
    "\n",
    "  shape ,scale = softmax_out(shape, scale, logits)\n",
    "  #pdb.set_trace()\n",
    "  #T = torch.tensor([*range(int(torch.min(t)), int(torch.max(t)))]).clone().detach()\n",
    "  T = torch.tensor(torch.arange(0.001,torch.max(t),1)).clone().detach()\n",
    "  #T = torch.tensor(np.arange(0.01,torch.max(t),0.01)).clone().detach()\n",
    "  surv_auc = []\n",
    "  for i in range(shape.shape[0]):\n",
    "    shape_temp = shape[i].repeat(len(T))\n",
    "    scale_temp = scale[i].repeat(len(T))\n",
    "    s = lognormal_surv( T.cuda(), shape_temp, scale_temp)\n",
    "    surv_auc.append(torch.trapz(s.cuda(),T.cuda()))\n",
    "  #pdb.set_trace()\n",
    "  survival_auc = torch.stack(surv_auc, dim=0)\n",
    "  return survival_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from survci.utilities import softmax_out, sample_lognormal, sample_weibull\n",
    "from lifelines.utils import concordance_index\n",
    "cis=[]\n",
    "ci_tr=[]\n",
    "ci_co=[]\n",
    "alphas=[]\n",
    "betas=[]\n",
    "# 95% confidence intervals\n",
    "def confidence_interval(metric):\n",
    "    lower =  np.percentile(metric, 2.5)\n",
    "    median = np.percentile(metric, 50)\n",
    "    upper =  np.percentile(metric, 97.5)  \n",
    "    return np.round(median,4),np.round(lower,4),np.round(upper,4)\n",
    "test_list = [test_list[0] for i in range(len(model_paths))]\n",
    "names = [names[0] for i in range(len(model_paths))]\n",
    "\n",
    "for i,j,k in zip(model_paths,test_list,names):\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(j[1])\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    # test\n",
    "    tf_test = np.array(j[0]['y_f'])\n",
    "    tcf_test = np.array(j[0]['y_cf'])\n",
    "    ef_test = np.array(j[0]['e_f'])\n",
    "    ecf_test = np.array(j[0]['e_cf'])\n",
    "    W_test = j[2]\n",
    "    model_test=load_model(i)\n",
    "    pehe_all = []\n",
    "    ci_all = []\n",
    "    ci_tr_all = []\n",
    "    ci_co_all = []\n",
    "    emp_ate_all = []\n",
    "    pred_ate_all = []\n",
    "    epsilon_ate_all = []\n",
    "    emp_ite_a = tf_test[W_test==1] - tcf_test[W_test==1]\n",
    "    emp_ite_b = tcf_test[W_test==0] - tf_test[W_test==0] \n",
    "    emp_ite = np.concatenate((emp_ite_a, emp_ite_b), axis=0)\n",
    "    emp_ate = np.mean(emp_ite)\n",
    "    for _ in range(100):\n",
    "        ind = np.random.randint(0, x_test.shape[0], x_test.shape[0])\n",
    "        yf = tf_test[ind]\n",
    "        ycf = tcf_test[ind]\n",
    "        x = x_test[ind]\n",
    "        w = W_test[ind]\n",
    "        ef = ef_test[ind]\n",
    "        treated_idx = np.where(w>0)[0]                        \n",
    "        control_idx = np.where(w<1)[0]\n",
    "        #Ground Truth Individual Treatment Effect (ITE)\n",
    "        # emp_ite_a = yf[w==1] - ycf[w==1]\n",
    "        # emp_ite_b = ycf[w==0] - yf[w==0] \n",
    "        # emp_ite = np.concatenate((emp_ite_a, emp_ite_b), axis=0)\n",
    "\n",
    "        #Distibution Parameters\n",
    "        shapef_co,scalef_co,logitsf_co, shapef_tr,scalef_tr,logitsf_tr = model_test.forward(torch.tensor(x).cuda(),torch.from_numpy(w).cuda())  #Predicted Factual Parameters \n",
    "\n",
    "        shapecf_co,scalecf_co,logitscf_co, shapecf_tr,scalecf_tr,logitscf_tr = model_test.forward(torch.tensor(x).cuda(),torch.from_numpy(1-w).cuda())  #Predicted Counter Factual Parameters \n",
    "\n",
    "        # #Factual and CounterFactual Outcomes \n",
    "        t_pred_f_co = auc( torch.from_numpy(yf[w==0]),shapef_co,scalef_co, logitsf_co ) #Predicted Factual Outcome (Treatment (w) = 0)\n",
    "        t_pred_f_tr = auc(torch.from_numpy(yf[w==1]),shapef_tr,scalef_tr, logitsf_tr) #Predicted Factual Outcome (Treatment(W) = 1)\n",
    "        t_pred_cf_co = auc( torch.from_numpy(ycf[w==1]),shapecf_co,scalecf_co,logitscf_co ) #Predcited CounterFactual Outcome (Treatment (W)= 1 )\n",
    "        t_pred_cf_tr = auc(torch.from_numpy(ycf[w==0]),shapecf_tr,scalecf_tr,logitscf_tr) #Predicted CounterFactual Outcome (Treatment (w)= 0)\n",
    "\n",
    "        #Predicted ITE \n",
    "        pred_ite_a = t_pred_f_tr.cpu().detach().numpy() - t_pred_cf_co.cpu().detach().numpy() #Factual_Treatment=1\n",
    "        pred_ite_b = t_pred_cf_tr.cpu().detach().numpy() - t_pred_f_co.cpu().detach().numpy() #Factual_Treatment=0\n",
    "        pred_ite = np.concatenate((pred_ite_a, pred_ite_b), axis=0)\n",
    "\n",
    "        # emp_ate = np.mean(emp_ite)\n",
    "        pred_ate =np.mean(pred_ite)\n",
    "        epsilon_ate = np.abs(emp_ate-pred_ate)\n",
    "        emp_ate_all.append(emp_ate)\n",
    "        pred_ate_all.append(pred_ate)\n",
    "        epsilon_ate_all.append(epsilon_ate)\n",
    "\n",
    "        #PEHE Metric\n",
    "        pehe = (emp_ite-pred_ite)**2\n",
    "        pehe_all.append(np.round(np.sqrt(np.mean(pehe)), 4))  #\\epsilon_{pehe}\n",
    "\n",
    "        #CI\n",
    "        c_index_co = concordance_index(event_times=yf[control_idx],predicted_scores=t_pred_f_co.cpu().detach().numpy(),event_observed=ef[control_idx])\n",
    "        c_index_tr = concordance_index(event_times=yf[treated_idx],predicted_scores=t_pred_f_tr.cpu().detach().numpy(),event_observed=ef[treated_idx])\n",
    "        ci_index = (c_index_co + c_index_tr) * 0.5\n",
    "        ci_all.append(ci_index)\n",
    "        ci_tr_all.append(c_index_tr)\n",
    "        ci_co_all.append(c_index_co)\n",
    "    print(i)\n",
    "    median,lower,upper = confidence_interval(pehe_all)\n",
    "    print('PEHE:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(ci_co_all)\n",
    "    print('CI Index - Control:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(ci_tr_all)\n",
    "    print('CI Index - Treated:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(ci_all)\n",
    "    print('Avg CI:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(emp_ate_all)\n",
    "    print('Ground Truth ATE:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(pred_ate_all)\n",
    "    print('Predicted ATE:', \"{} ({},{})\".format(median,lower, upper))\n",
    "    median,lower,upper = confidence_interval(epsilon_ate_all)\n",
    "    print('Epsilon ATE:', \"{} ({},{})\".format(median,lower, upper))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of surv_ci_example.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1e9e04228cc62b6d88a2d49485c7cafe926fad2c85f5988ecce6180f5d2d1170"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
