{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/muskang48/SurvCI/blob/main/surv_ci_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhaWjhU0nOQi",
    "outputId": "4ceefb89-0b62-4e75-a267-445cb4b5e3c6"
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvCI_dir = os.getcwd()\n",
    "data_dir = SurvCI_dir + \"/data/\"\n",
    "actg_dir = data_dir + \"ACTG Data/\"\n",
    "Synthetic_SurvITE_dir = data_dir + \"Synthetic_SurvITE/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "S-3 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train = np.load(Synthetic_SurvITE_dir + \"train_s3_[0, 1]_1.npz\")\n",
    "s3_test = np.load(Synthetic_SurvITE_dir + \"test_s3_[0, 1]_1.npz\") \n",
    "\n",
    "train_list = [s3_train]\n",
    "test_list = [s3_test]\n",
    "names = [\"s3_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-4 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_train = np.load(Synthetic_SurvITE_dir + \"train_s4_[0, 1]_1.npz\")\n",
    "s4_test = np.load(Synthetic_SurvITE_dir + \"test_s4_[0, 1]_1.npz\") \n",
    "\n",
    "train_list = [s4_train]\n",
    "test_list = [s4_test]\n",
    "names = [\"s4_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-1 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = np.load(Synthetic_SurvITE_dir+\"train_s1.npz\")\n",
    "s1_test = np.load(Synthetic_SurvITE_dir+\"test_s1.npz\")\n",
    "\n",
    "train_list = [s1_train]\n",
    "test_list = [s1_test]\n",
    "names = [\"s1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S-2 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_train = np.load(Synthetic_SurvITE_dir+\"train_s2.npz\")\n",
    "s2_test = np.load(Synthetic_SurvITE_dir+\"test_s2.npz\")\n",
    "\n",
    "train_list = [s2_train]\n",
    "test_list = [s2_test]\n",
    "names = [\"s2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actg_s3_train_gt = pd.read_csv(actg_dir+\"s3/train/event_pairs.csv\")\n",
    "actg_s3_test_gt = pd.read_csv(actg_dir+\"s3/test/event_pairs.csv\")\n",
    "actg_s3_val_gt = pd.read_csv(actg_dir+\"s3/val/event_pairs.csv\")\n",
    "actg_s3_train_x = np.load(actg_dir+\"s3/train/covariates.npy\")\n",
    "actg_s3_test_x = np.load(actg_dir+\"s3/test/covariates.npy\")\n",
    "actg_s3_val_x = np.load(actg_dir+\"s3/val/covariates.npy\")\n",
    "actg_s3_train_a = np.load(actg_dir+\"s3/train/treatment.npy\")\n",
    "actg_s3_test_a = np.load(actg_dir+\"s3/test/treatment.npy\")\n",
    "actg_s3_val_a = np.load(actg_dir+\"s3/val/treatment.npy\")\n",
    "\n",
    "train_list = [[actg_s3_train_gt,actg_s3_train_x,actg_s3_train_a]]\n",
    "test_list = [[actg_s3_test_gt,actg_s3_test_x,actg_s3_test_a]]\n",
    "val_list = [[actg_s3_val_gt,actg_s3_val_x,actg_s3_val_a]]\n",
    "names = [\"actg_s3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actg_s4_train_gt = pd.read_csv(actg_dir+\"s4/train/event_pairs.csv\")\n",
    "actg_s4_test_gt = pd.read_csv(actg_dir+\"s4/test/event_pairs.csv\")\n",
    "actg_s4_val_gt = pd.read_csv(actg_dir+\"s4/val/event_pairs.csv\")\n",
    "actg_s4_train_x = np.load(actg_dir+\"s4/train/covariates.npy\")\n",
    "actg_s4_test_x = np.load(actg_dir+\"s4/test/covariates.npy\")\n",
    "actg_s4_val_x = np.load(actg_dir+\"s4/val/covariates.npy\")\n",
    "actg_s4_train_a = np.load(actg_dir+\"s4/train/treatment.npy\")\n",
    "actg_s4_test_a = np.load(actg_dir+\"s4/test/treatment.npy\")\n",
    "actg_s4_val_a = np.load(actg_dir+\"s4/val/treatment.npy\")\n",
    "\n",
    "train_list = [[actg_s4_train_gt,actg_s4_train_x,actg_s4_train_a]]\n",
    "test_list = [[actg_s4_test_gt,actg_s4_test_x,actg_s4_test_a]]\n",
    "val_list = [[actg_s4_val_gt,actg_s4_val_x,actg_s4_val_a]]\n",
    "names = [\"actg_s4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwZsXyIlrbFY"
   },
   "source": [
    "\n",
    "\n",
    "# SurvCI Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlPz3INDnve0",
    "outputId": "1771816d-dd74-47c9-8415-762212c1fe35"
   },
   "outputs": [],
   "source": [
    "%cd \"{SurvCI_dir}/survci\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C7T5sHfGnxKi"
   },
   "outputs": [],
   "source": [
    "#Importing Model\n",
    "from survci.survci_api import survciBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG data Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "#k,p_alpha,p_beta,\n",
    "p_alpha=[0.5,0.6,0.7]\n",
    "p_beta=[1e-4,2e-4,3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "k = [3]\n",
    "\n",
    "for i,j,k,l in zip(train_list,test_list,val_list,names):\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(i[1])\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(j[1])\n",
    "    x_val = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(k[1])\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    x_val = StandardScaler().fit_transform(x_val)\n",
    "    # train\n",
    "    tf_train = i[0]['y_f']\n",
    "    tcf_train = i[0]['y_cf']\n",
    "    ef_train = i[0]['e_f']\n",
    "    ecf_train = i[0]['e_cf']\n",
    "    W_train = i[2]\n",
    "    # test\n",
    "    tf_test = j[0]['y_f']\n",
    "    tcf_test = j[0]['y_cf']\n",
    "    ef_test = j[0]['e_f']\n",
    "    ecf_test = j[0]['e_cf']\n",
    "    W_test = j[2]\n",
    "    # val\n",
    "    tf_val = k[0]['y_f']\n",
    "    tcf_val = k[0]['y_cf']\n",
    "    ef_val = k[0]['e_f']\n",
    "    ecf_val = k[0]['e_cf']\n",
    "    W_val = k[2]\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    ci=[]\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=l)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, np.array(tf_train), np.array(ef_train),W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, np.array(tf_val), np.array(ef_val),W_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JB-HSY2d1g4I",
    "outputId": "3d61fe0c-9eb9-4ceb-e00f-7ccf8b8cb835"
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "#k,p_alpha,p_beta,\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j[\"t\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_test))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    rand3 = np.random.randint(0,len(W_val),20)\n",
    "    rand4 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j,k,l in zip(rand1,rand2,rand3,rand4):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "        ef_val[k] = 0\n",
    "        ef_val[l] = 0\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e\"]\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j[\"t\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e\"]\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    # tf_train = i['t']\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t_f']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S4 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "p_alpha=[0.6]\n",
    "p_beta=[3e-4]\n",
    "p_lamda = [2e-1]\n",
    "p_gamma = [1e-1]\n",
    "\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e_f\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t_f']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e_f\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_test))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    param_grid = {'k' : [3],    #Number of Parametric Distibutions\n",
    "                'distribution' : ['LogNormal'],   \n",
    "                'learning_rate' : [3e-4],\n",
    "                'layers' : [[100,100]],\n",
    "                'discount': [1],     #(Scaling ELBO_censored Loss)\n",
    "                'imb_func': ['mmd2_lin'],  \n",
    "                'p_alpha': p_alpha,       #Hyperparameter for Scaling IPM-Imbalance\n",
    "                'p_beta': p_beta,   #Hyperparameter for Scaling MSE Loss\n",
    "                'p_gamma':p_gamma, #Hyperparameter for Scaling ELBO Loss\n",
    "                'p_lamda': p_lamda   #Hyperparameter for scaling Regularization Loss\n",
    "                }\n",
    "    params = ParameterGrid(param_grid)\n",
    "    print(x_train.shape)\n",
    "    for param in params:\n",
    "        model = survciBase(k = param['k'],\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_lambda=param[\"p_lamda\"],name=k)\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train, tf_train, ef_train,W_train, iters = 200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val, tf_val, ef_val,W_val))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of surv_ci_example.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1e9e04228cc62b6d88a2d49485c7cafe926fad2c85f5988ecce6180f5d2d1170"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
