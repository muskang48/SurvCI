{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/muskang48/SurvCI/blob/main/surv_ci_info_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22IQSM9SZqLm",
    "outputId": "738bde72-3f27-4cb1-f603-ff64a188eeb0",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Libraries \n",
    "# !pip install lifelines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# s4_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_0.33.npz\")\n",
    "# # s4_train_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_0.npz\")\n",
    "# s4_train_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[0, 1]_1.npz\")\n",
    "# s4_train_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_0.33.npz\")\n",
    "# # s4_train_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_0.npz\")\n",
    "# s4_train_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s4_[8, 9]_1.npz\")\n",
    "\n",
    "# s4_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_0.33.npz\")\n",
    "# # s4_test_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_0.npz\")\n",
    "# s4_test_2 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[0, 1]_1.npz\")\n",
    "# s4_test_3 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_0.33.npz\")\n",
    "# # s4_test_5 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_0.npz\")\n",
    "# s4_test_4 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s4_[8, 9]_1.npz\")\n",
    "\n",
    "# train_list = [s4_train_2]\n",
    "# test_list = [s4_test_2]\n",
    "# names = [\"s4_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# s2_train_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/train_s2.npz\")\n",
    "# s2_test_1 = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/survite_data/test_s2.npz\")\n",
    "\n",
    "# train_list = [s2_train_1]\n",
    "# test_list = [s2_test_1]\n",
    "# names = [\"s2_[0, 1]_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG_S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# actg_s4_train_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/event_pairs.csv\")\n",
    "# actg_s4_test_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/event_pairs.csv\")\n",
    "# actg_s4_val_gt = pd.read_csv(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/event_pairs.csv\")\n",
    "# actg_s4_train_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/covariates.npy\")\n",
    "# actg_s4_test_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/covariates.npy\")\n",
    "# actg_s4_val_x = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/covariates.npy\")\n",
    "# actg_s4_train_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/train/treatment.npy\")\n",
    "# actg_s4_test_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/test/treatment.npy\")\n",
    "# actg_s4_val_a = np.load(\"/home/iiitd/gokul/intern/SurvCI/data/ACTG Data/s4/val/treatment.npy\")\n",
    "\n",
    "# train_list = [[actg_s4_train_gt,actg_s4_train_x,actg_s4_train_a]]\n",
    "# test_list = [[actg_s4_test_gt,actg_s4_test_x,actg_s4_test_a]]\n",
    "# val_list = [[actg_s4_val_gt,actg_s4_val_x,actg_s4_val_a]]\n",
    "# names = [\"actg_s4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zie1EIrSbLB9",
    "outputId": "bb07b151-fa9e-4cfa-e0eb-df301b33947a",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iiitd/gokul/intern/SurvCI/surv_ci_info\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/surv_ci_info\n",
    "%cd /home/iiitd/gokul/intern/SurvCI/surv_ci_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sc6F8v9gbUli",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#Importing Model\n",
    "from surv_ci_info.survci_info_api import survci_infoBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTG DATA TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "p_alpha=[2e-2]\n",
    "p_beta=[7e-6]\n",
    "p_gamma=[1]\n",
    "\n",
    "for i,j,k,l in zip(train_list,test_list,val_list,names):\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(i[1])\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(j[1])\n",
    "    x_val = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(k[1])\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    x_val = StandardScaler().fit_transform(x_val)\n",
    "    # train\n",
    "    tf_train = np.array(i[0]['y_f'])\n",
    "    tcf_train = np.array(i[0]['y_cf'])\n",
    "    ef_train = np.array(i[0]['e_f'])\n",
    "    ecf_train = np.array(i[0]['e_cf'])\n",
    "    W_train = np.array(i[2])\n",
    "    # test\n",
    "    tf_test = j[0]['y_f']\n",
    "    tcf_test = j[0]['y_cf']\n",
    "    ef_test = j[0]['e_f']\n",
    "    ecf_test = j[0]['e_cf']\n",
    "    W_test = j[2]\n",
    "    # val\n",
    "    tf_val = np.array(k[0]['y_f'])\n",
    "    tcf_val = np.array(k[0]['y_cf'])\n",
    "    ef_val = np.array(k[0]['e_f'])\n",
    "    ecf_val = np.array(k[0]['e_cf'])\n",
    "    W_val = k[2]\n",
    "    param_grid = {'k' : [10], #6,10\n",
    "            'distribution' : ['LogNormal'],\n",
    "            'learning_rate' : [3e-4], #3e-4\n",
    "            'layers' : [[100,100]],  #[100,100],[300,300], [300,100]\n",
    "            'discount': [1],   #Censoring\n",
    "            'imb_func': ['mmd2_lin'],\n",
    "            'p_alpha': p_alpha,  #Scaling IPM -Imbalance 2e-2,5e-1\n",
    "            'p_beta': p_beta,  #Scaling MSE 7e-6,1e-3,1e-5\n",
    "            'p_gamma': p_gamma, #Scaling ELBO Loss 1e-1\n",
    "            'p_lamda' : [1e-3], #Scaling Regularization Loss 1.5e-1\n",
    "            }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    for param in params:\n",
    "        model = survci_infoBase(k = param['k'],name=l,\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_gamma = param['p_gamma'],p_lamda =param['p_lamda'])\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train,tf_train,ef_train,W_train,iters=200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val,tf_val,ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S1  Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q8yyyIVVblVh",
    "outputId": "73ec6266-43ba-42eb-beff-32dfaac2a495",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "p_alpha=[10]\n",
    "p_beta=[3e-5]\n",
    "p_gamma=[1,0.6]\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t']\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = np.array([1 for i in range(len(W_test))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    rand3 = np.random.randint(0,len(W_val),20)\n",
    "    rand4 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j,k,l in zip(rand1,rand2,rand3,rand4):\n",
    "        ef_train[i] = 0\n",
    "        W_train[j] = 1\n",
    "        ef_val[k] = 0\n",
    "        W_val[l] = 1\n",
    "    param_grid = {'k' : [3], #6,10\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [3e-4], #3e-4\n",
    "              'layers' : [[100,100]],  #[100,100],[300,300], [300,100]\n",
    "              'discount': [1],   #Censoring\n",
    "              'imb_func': ['mmd2_lin'],\n",
    "              'p_alpha': p_alpha,  #Scaling IPM -Imbalance 2e-2,5e-1\n",
    "              'p_beta': p_beta,  #Scaling MSE 7e-6,1e-3,1e-5\n",
    "              'p_gamma': p_gamma, #Scaling ELBO Loss 1e-1\n",
    "              'p_lamda' : [0.2], #Scaling Regularization Loss 1.5e-1\n",
    "             }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    for param in params:\n",
    "        model = survci_infoBase(k = param['k'],name=k,\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_gamma = param['p_gamma'],p_lamda =param['p_lamda'])\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train,tf_train,ef_train,W_train,iters=200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val,tf_val,ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S2  Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "p_alpha=[10]\n",
    "p_beta=[3e-5]\n",
    "p_gamma=[1,0.6]\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t']\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e\"]\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t']\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e\"]\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        W_val[i] = 1\n",
    "        W_train[j] = 1\n",
    "    param_grid = {'k' : [3], #6,10\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [3e-4], #3e-4\n",
    "              'layers' : [[100,100]],  #[100,100],[300,300], [300,100]\n",
    "              'discount': [1],   #Censoring\n",
    "              'imb_func': ['mmd2_lin'],\n",
    "              'p_alpha': p_alpha,  #Scaling IPM -Imbalance 2e-2,5e-1\n",
    "              'p_beta': p_beta,  #Scaling MSE 7e-6,1e-3,1e-5\n",
    "              'p_gamma': p_gamma, #Scaling ELBO Loss 1e-1\n",
    "              'p_lamda' : [0.2], #Scaling Regularization Loss 1.5e-1\n",
    "             }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    for param in params:\n",
    "        model = survci_infoBase(k = param['k'],name=k,\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_gamma = param['p_gamma'],p_lamda =param['p_lamda'])\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train,tf_train,ef_train,W_train,iters=200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val,tf_val,ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "p_alpha=[10]\n",
    "p_beta=[3e-5]\n",
    "p_gamma=[1,0.6]\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_train = np.array([1 for i in range(len(W_train))])\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    rand1 = np.random.randint(0,len(W_val),20)\n",
    "    rand2 = np.random.randint(0,len(W_val),20)\n",
    "    for i,j in zip(rand1,rand2):\n",
    "        ef_train[i] = 0\n",
    "        ef_val[j] = 0\n",
    "    param_grid = {'k' : [3], #6,10\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [3e-4], #3e-4\n",
    "              'layers' : [[100,100]],  #[100,100],[300,300], [300,100]\n",
    "              'discount': [1],   #Censoring\n",
    "              'imb_func': ['mmd2_lin'],\n",
    "              'p_alpha': p_alpha,  #Scaling IPM -Imbalance 2e-2,5e-1\n",
    "              'p_beta': p_beta,  #Scaling MSE 7e-6,1e-3,1e-5\n",
    "              'p_gamma': p_gamma, #Scaling ELBO Loss 1e-1\n",
    "              'p_lamda' : [0.2], #Scaling Regularization Loss 1.5e-1\n",
    "             }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    for param in params:\n",
    "        model = survci_infoBase(k = param['k'],name=k,\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_gamma = param['p_gamma'],p_lamda =param['p_lamda'])\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train,tf_train,ef_train,W_train,iters=200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val,tf_val,ef_val,W_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic S4 Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "p_alpha=[10]\n",
    "p_beta=[3e-5]\n",
    "p_gamma=[1,0.6]\n",
    "for i,j,k in zip(train_list,test_list,names):\n",
    "    x_train = i[\"x\"]\n",
    "    x_train = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_train)\n",
    "    x_train = StandardScaler().fit_transform(x_train)\n",
    "    tf_train = i['t_f']\n",
    "    tcf_train = i[\"t_cf\"]\n",
    "    W_train = i[\"a\"]\n",
    "    ef_train = i[\"e_f\"]\n",
    "    ecf_train = i[\"e_cf\"]\n",
    "\n",
    "    x_test = j[\"x\"]\n",
    "    x_test = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(x_test)\n",
    "    x_test = StandardScaler().fit_transform(x_test)\n",
    "    tf_test = j['t']\n",
    "    tcf_test = j[\"t_cf\"]\n",
    "    W_test = j[\"a\"]\n",
    "    ef_test = j[\"e_f\"]\n",
    "    ecf_test = j[\"e_cf\"]\n",
    "\n",
    "    n = len(x_train)\n",
    "    tr_size = int(n*0.70)\n",
    "    vl_size = int(n*0.30)\n",
    "\n",
    "    x_train,x_val = x_train[:tr_size], x_train[tr_size:tr_size+vl_size]\n",
    "    tf_train,tf_val = tf_train[:tr_size], tf_train[tr_size:tr_size+vl_size]\n",
    "    ef_train,ef_val = ef_train[:tr_size], ef_train[tr_size:tr_size+vl_size]\n",
    "    tcf_train,tcf_val = tcf_train[:tr_size], tcf_train[tr_size:tr_size+vl_size]\n",
    "    W_train,W_val = W_train[:tr_size], W_train[tr_size:tr_size+vl_size]\n",
    "    param_grid = {'k' : [3], #6,10\n",
    "              'distribution' : ['LogNormal'],\n",
    "              'learning_rate' : [3e-4], #3e-4\n",
    "              'layers' : [[100,100]],  #[100,100],[300,300], [300,100]\n",
    "              'discount': [1],   #Censoring\n",
    "              'imb_func': ['mmd2_lin'],\n",
    "              'p_alpha': p_alpha,  #Scaling IPM -Imbalance 2e-2,5e-1\n",
    "              'p_beta': p_beta,  #Scaling MSE 7e-6,1e-3,1e-5\n",
    "              'p_gamma': p_gamma, #Scaling ELBO Loss 1e-1\n",
    "              'p_lamda' : [0.2], #Scaling Regularization Loss 1.5e-1\n",
    "             }\n",
    "    params = ParameterGrid(param_grid)\n",
    "\n",
    "    models = []\n",
    "    for param in params:\n",
    "        model = survci_infoBase(k = param['k'],name=k,\n",
    "                                    distribution = param['distribution'],\n",
    "                                    layers = param['layers'],\n",
    "                                    discount=param['discount'],\n",
    "                                    imb_func=param['imb_func'],p_alpha=param['p_alpha'],p_beta = param['p_beta'],p_gamma = param['p_gamma'],p_lamda =param['p_lamda'])\n",
    "        # The fit method is called to train the model\n",
    "        model.fit(x_train,tf_train,ef_train,W_train,iters=200,learning_rate = param['learning_rate'],batch_size=3500,val_data=(x_val,tf_val,ef_val,W_val))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMJdLFm25M6/bnbt2uYU+r",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1soHiukz7DmehncSyAFbIRNumAawxuxpi",
   "name": "surv_ci_info_example.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1e9e04228cc62b6d88a2d49485c7cafe926fad2c85f5988ecce6180f5d2d1170"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
